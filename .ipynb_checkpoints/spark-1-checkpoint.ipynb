{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da4709c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccc28de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0bfe2ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8124/3439460170.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Practise'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                         \u001b[1;31m# This SparkContext may be an existing one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m                         \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m                     \u001b[1;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                     \u001b[1;31m# by all sessions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName('Practise').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c651ed73",
   "metadata": {},
   "source": [
    "# Load data - read.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f91d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_pyspark=spark.read.csv('datasets/cal_housing.csv')\n",
    "# we should use inferSchema=True to get correct types, or all types will be string\n",
    "# df_pyspark=spark.read.option('header','true').csv('datasets/cal_housing.csv',inferSchema=True)\n",
    "# or\n",
    "df_pyspark=spark.read.csv('datasets/cal_housing.csv',header=True,inferSchema=True)\n",
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef4077",
   "metadata": {},
   "source": [
    "# set columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472b2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_pyspark.toDF('longitude','latitude','housingMedianAge','totalRooms','totalBedrooms','population','households',\n",
    "                   'medianIncome','medianHouseValue' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2854f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7806e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52567c0",
   "metadata": {},
   "source": [
    "# To check the data types of the columns (Schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a3f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cdc1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d861b47e",
   "metadata": {},
   "source": [
    "# Select few columns to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(['medianIncome','medianHouseValue']).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd0358",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a4901",
   "metadata": {},
   "source": [
    "# Adding new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e921ea3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note: this is not in place operation we need to assign it to a variable\n",
    "df=df.withColumn('population After 2 yesrs',df['population']+1000)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e866eca",
   "metadata": {},
   "source": [
    "# Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a47a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df=df.drop('population After 2 yesrs','longitude','latitude')\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92e546d",
   "metadata": {},
   "source": [
    "# Rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53015a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=df.withColumnRenamed('medianHouseValue','Median_House_Value')\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32af5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all null values\n",
    "df.na.drop();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2de2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop using 'how'\n",
    "# all will drop if all row is null, and any will drop if any is null \n",
    "df.na.drop(how='all');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82afd460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh=2 will keep the row if there is two not null values in the row  \n",
    "df.na.drop(how='all',thresh=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7744ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset will delete any rows if the value in spicefied column is null \n",
    "df.na.drop(how='all',subset=['Median_House_Value']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b33fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling the missing value, this will fill the missing value with 0 \n",
    "df.na.fill(0,['totalRooms','totalBedrooms']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f04600d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill using Imputer function, will fill missing values with mean\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer=Imputer(\n",
    "inputCols=['totalRooms','totalBedrooms'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['totalRooms','totalBedrooms']],\n",
    "    ).setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01071373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add imputation cols to df\n",
    "imputer.fit(df).transform(df).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2551577",
   "metadata": {},
   "source": [
    "# Filter Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d11e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median_House_Value grater than or equl to 500,000\n",
    "df.filter('Median_House_Value>=500000').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting few columns\n",
    "df.filter(df['Median_House_Value']<=30000).select(['medianIncome','Median_House_Value']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a800c177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple conditions\n",
    "df.filter((df['Median_House_Value']>=500000) & \n",
    "          (df['totalBedrooms']<=5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38427bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (~) inverse operation (not), this will show anything not matching our conditions\n",
    "df.filter(~(df['Median_House_Value']>=500000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b662426",
   "metadata": {},
   "source": [
    "# Group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a388ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('housingMedianAge').sum('Median_House_Value').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6f382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4dd03b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada8ba57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
